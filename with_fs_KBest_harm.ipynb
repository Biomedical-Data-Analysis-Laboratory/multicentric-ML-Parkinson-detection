{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b36d481",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from imblearn.metrics import specificity_score\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97701753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "\n",
    "data = pd.read_csv('bm_combat_spectral_changed.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfa3e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# harmonized \n",
    "data_comb = data.drop(data.iloc[:, 209:], axis=1)\n",
    "data_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "X = data_log\n",
    "y = data_log[['center','label']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    train_size=0.7,\n",
    "                                                    random_state=1,\n",
    "                                                    stratify=X[['center','group']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab9aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_names = ['all','california','finland','iowa','medellin']\n",
    "\n",
    "X_train_sets = {} \n",
    "y_train_sets = {}\n",
    "X_test_sets = {}\n",
    "y_test_sets = {}\n",
    "\n",
    "\n",
    "for name in center_names:\n",
    "    \n",
    "    if name == 'all':\n",
    "        X_train_sets.update({name: X_train.drop(['center', 'group', 'age', 'gender', 'batch', 'label'], axis=1)})\n",
    "        X_test_sets.update({name: X_test.drop(['center', 'group', 'age', 'gender', 'batch', 'label'], axis=1)})\n",
    "        y_train_sets.update({name: y_train.drop(['center'], axis = 1)})\n",
    "        y_test_sets.update({name: y_test.drop(['center'], axis = 1)})\n",
    "    \n",
    "    else: \n",
    "        X_train_set = X_train.loc[X_train['center'] == name]\n",
    "        X_train_set = X_train_set.drop(['center', 'group', 'age', 'gender', 'batch', 'label'], axis=1)\n",
    "        X_train_sets.update({name: X_train_set})\n",
    "\n",
    "        y_train_set = y_train.loc[X_train['center'] == name]\n",
    "        y_train_set = y_train_set.drop(['center'], axis=1)\n",
    "        y_train_sets.update({name: y_train_set})\n",
    "\n",
    "        X_test_set = X_test.loc[X_test['center'] == name]\n",
    "        X_test_set = X_test_set.drop(['center', 'group', 'age', 'gender', 'batch', 'label'], axis=1)\n",
    "        X_test_sets.update({name: X_test_set})\n",
    "\n",
    "        y_test_set = y_test.loc[X_test['center'] == name]\n",
    "        y_test_set = y_test_set.drop(['center'], axis=1)\n",
    "        y_test_sets.update({name: y_test_set})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b8b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Classifiers\n",
    "\n",
    "clf1 = LogisticRegression(multi_class='multinomial',\n",
    "                          solver='newton-cg',\n",
    "                          random_state=1)\n",
    "\n",
    "clf2 = KNeighborsClassifier(algorithm='ball_tree',\n",
    "                            leaf_size=50)\n",
    "\n",
    "clf3 = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "clf4 = SVC(random_state=1)\n",
    "\n",
    "# Building the pipelines\n",
    "\n",
    "pipe1 = Pipeline([('clf1', clf1)])\n",
    "\n",
    "pipe2 = Pipeline([('clf2', clf2)])\n",
    "\n",
    "pipe4 = Pipeline([('clf4', clf4)])\n",
    "\n",
    "\n",
    "# Setting up the parameter grids\n",
    "param_grid1 = [{'clf1__penalty': ['l2'],\n",
    "                'clf1__C': np.power(10., np.arange(-4, 4))}]\n",
    "\n",
    "param_grid2 = [{'clf2__n_neighbors': list(range(1, 10)),\n",
    "                'clf2__p': [1, 2]}]\n",
    "\n",
    "param_grid3 = [{'max_depth': list(range(1, 10)) + [None],\n",
    "                'criterion': ['gini', 'entropy']}]\n",
    "\n",
    "\n",
    "param_grid4 = [\n",
    "               {'clf4__kernel': ['rbf'],\n",
    "                'clf4__C': np.power(10., np.arange(-4, 4)),\n",
    "                'clf4__gamma': np.power(10., np.arange(-5, 0))},\n",
    "               {'clf4__kernel': ['linear'],\n",
    "                'clf4__C': np.power(10., np.arange(-4, 4))}\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a446dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up multiple GridSearchCV objects, 1 for each algorithm\n",
    "\n",
    "\n",
    "gridcvs = {}\n",
    "\n",
    "for pgrid, est, name in zip((param_grid1, param_grid2,\n",
    "                             param_grid3, param_grid4),\n",
    "                            (pipe1, pipe2, clf3, pipe4),\n",
    "                            ('Softmax', 'KNN', 'DTree', 'SVM')):\n",
    "    \n",
    "    gcv = GridSearchCV(estimator=est,\n",
    "                       param_grid=pgrid,\n",
    "                       scoring='accuracy',\n",
    "                       n_jobs=1,\n",
    "                       cv=5,\n",
    "                       verbose=0,\n",
    "                       refit=True)\n",
    "    gridcvs[name] = gcv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ef3b1",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION METHOD: SELECTKBEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba6dfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_k = np.linspace(1, 203, num=203, dtype=int)\n",
    "list_of_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc5f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy','recall','specificity','precision','F1','auc']\n",
    "folds = [1, 2, 3, 4, 5]\n",
    "feature_scoring = {k: {center_name: [] for center_name in center_names} for k in list_of_k}\n",
    "cv_scores = {k: {center_name: {name: {metric: [] for metric in metrics} for name, gs_est in gridcvs.items()} for center_name in center_names} for k in list_of_k}\n",
    "cv_scores_without_zeros = {k: {center_name: {name: {metric: [] for metric in metrics} for name, gs_est in gridcvs.items()} for center_name in center_names} for k in list_of_k}\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "features_ids = {k: {center_name: [] for center_name in center_names} for k in list_of_k}\n",
    "features_names = {k: {center_name: [] for center_name in center_names} for k in list_of_k}\n",
    "\n",
    "for k in list_of_k:\n",
    "\n",
    "    for center_name in center_names:\n",
    "        \n",
    "        if center_name != center_names[0]: continue\n",
    "\n",
    "        print('Cross-validation + feature selection for number of features of ' + str(k) + ' for ' + center_name.upper())\n",
    "\n",
    "        # The outer loop for algorithm selection\n",
    "        c = 1\n",
    "        for outer_train_idx, outer_valid_idx in skfold.split(X_train_sets[center_name],y_train_sets[center_name]):\n",
    "\n",
    "            # feature selection: starts here\n",
    "\n",
    "            selector = SelectKBest(score_func=f_classif, k=k)\n",
    "\n",
    "            # run score function on (X,y) and get the appropriate features\n",
    "            fit = selector.fit(X_train_sets[center_name].iloc[outer_train_idx.tolist()], y_train_sets[center_name].iloc[outer_train_idx.tolist()].values.ravel())\n",
    "            \n",
    "            feature_scoring[k][center_name].append(fit.scores_)\n",
    "\n",
    "            # Get columns to keep and create new dataframe with those only\n",
    "            selected_features = fit.get_support(indices=True)\n",
    "\n",
    "            features = X_train_sets[center_name].columns[selected_features].to_list()\n",
    "            \n",
    "            features_ids[k][center_name].append(selected_features)\n",
    "            features_names[k][center_name].append(features)\n",
    "\n",
    "            # feature selection: ends here\n",
    "\n",
    "            for name, gs_est in sorted(gridcvs.items()):\n",
    "\n",
    "                print('outer fold %d/5 | tuning %-8s' % (c, name), end='')\n",
    "\n",
    "                # The inner loop for hyperparameter tuning\n",
    "\n",
    "                if len(features) != 0:\n",
    "\n",
    "                    gs_est.fit(X_train_sets[center_name].iloc[outer_train_idx.tolist()].iloc[:,selected_features], y_train_sets[center_name].iloc[outer_train_idx.tolist()].values.ravel())\n",
    "                    y_pred = gs_est.predict(X_train_sets[center_name].iloc[outer_valid_idx.tolist()].iloc[:,selected_features])\n",
    "\n",
    "\n",
    "                    for metric in metrics: \n",
    "\n",
    "                        if metric == 'accuracy':\n",
    "                            calc_metric = accuracy_score(y_true=y_train_sets[center_name].iloc[outer_valid_idx.tolist()], y_pred=y_pred)\n",
    "                            print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "                                  (gs_est.best_score_ * 100, calc_metric * 100))\n",
    "                            cv_scores[k][center_name][name][metric].append(calc_metric)\n",
    "                            if calc_metric != 0:\n",
    "                                cv_scores_without_zeros[k][center_name][name][metric].append(calc_metric)\n",
    "                        elif metric == 'recall':\n",
    "                            calc_metric = recall_score(y_true=y_train_sets[center_name].iloc[outer_valid_idx.tolist()], y_pred=y_pred)\n",
    "                            cv_scores[k][center_name][name][metric].append(calc_metric)\n",
    "                            if calc_metric != 0:\n",
    "                                cv_scores_without_zeros[k][center_name][name][metric].append(calc_metric)\n",
    "                        elif metric == 'specificity':\n",
    "                            calc_metric = specificity_score(y_true=y_train_sets[center_name].iloc[outer_valid_idx.tolist()], y_pred=y_pred)\n",
    "                            cv_scores[k][center_name][name][metric].append(calc_metric)\n",
    "                            if calc_metric != 0:\n",
    "                                cv_scores_without_zeros[k][center_name][name][metric].append(calc_metric)\n",
    "                        elif metric == 'precision':\n",
    "                            calc_metric = precision_score(y_true=y_train_sets[center_name].iloc[outer_valid_idx.tolist()], y_pred=y_pred)\n",
    "                            cv_scores[k][center_name][name][metric].append(calc_metric)\n",
    "                            if calc_metric != 0:\n",
    "                                cv_scores_without_zeros[k][center_name][name][metric].append(calc_metric)\n",
    "                        elif metric == 'F1':\n",
    "                            calc_metric = f1_score(y_true=y_train_sets[center_name].iloc[outer_valid_idx.tolist()], y_pred=y_pred)\n",
    "                            cv_scores[k][center_name][name][metric].append(calc_metric)\n",
    "                            if calc_metric != 0:\n",
    "                                cv_scores_without_zeros[k][center_name][name][metric].append(calc_metric)\n",
    "                        else:\n",
    "                            calc_metric = roc_auc_score(y_true=y_train_sets[center_name].iloc[outer_valid_idx.tolist()], y_score=y_pred, average = 'macro')\n",
    "                            cv_scores[k][center_name][name][metric].append(calc_metric)\n",
    "                            if calc_metric != 0:\n",
    "                                cv_scores_without_zeros[k][center_name][name][metric].append(calc_metric)\n",
    "                else:\n",
    "\n",
    "                    print('!!!NO FEATURES WERE SELECTED!!!')\n",
    "\n",
    "                    gs_est.fit(X_train_sets[center_name].iloc[outer_train_idx.tolist()], y_train_sets[center_name].iloc[outer_train_idx.tolist()].values.ravel())\n",
    "                    y_pred = gs_est.predict(X_train_sets[center_name].iloc[outer_valid_idx.tolist()])\n",
    "\n",
    "                    for metric in metrics: \n",
    "\n",
    "                        if metric == 'accuracy':\n",
    "                            calc_metric = accuracy_score(y_true=y_train_sets[center_name].iloc[outer_valid_idx.tolist()], y_pred=y_pred)\n",
    "                            print(' | inner ACC %.2f%% | outer ACC %.2f%%' %\n",
    "                                  (gs_est.best_score_ * 100, calc_metric * 100))\n",
    "                            cv_scores[k][center_name][name][metric].append(calc_metric)\n",
    "                            if calc_metric != 0:\n",
    "                                cv_scores_without_zeros[k][center_name][name][metric].append(calc_metric)\n",
    "                        elif metric == 'recall':\n",
    "                            calc_metric = recall_score(y_true=y_train_sets[center_name].iloc[outer_valid_idx.tolist()], y_pred=y_pred)\n",
    "                            cv_scores[k][center_name][name][metric].append(calc_metric)\n",
    "                            if calc_metric != 0:\n",
    "                                cv_scores_without_zeros[k][center_name][name][metric].append(calc_metric)\n",
    "                        elif metric == 'specificity':\n",
    "                            calc_metric = specificity_score(y_true=y_train_sets[center_name].iloc[outer_valid_idx.tolist()], y_pred=y_pred)\n",
    "                            cv_scores[k][center_name][name][metric].append(calc_metric)\n",
    "                            if calc_metric != 0:\n",
    "                                cv_scores_without_zeros[k][center_name][name][metric].append(calc_metric)\n",
    "                        elif metric == 'precision':\n",
    "                            calc_metric = precision_score(y_true=y_train_sets[center_name].iloc[outer_valid_idx.tolist()], y_pred=y_pred)\n",
    "                            cv_scores[k][center_name][name][metric].append(calc_metric)\n",
    "                            if calc_metric != 0:\n",
    "                                cv_scores_without_zeros[k][center_name][name][metric].append(calc_metric)\n",
    "                        elif metric == 'F1':\n",
    "                            calc_metric = f1_score(y_true=y_train_sets[center_name].iloc[outer_valid_idx.tolist()], y_pred=y_pred)\n",
    "                            cv_scores[k][center_name][name][metric].append(calc_metric)\n",
    "                            if calc_metric != 0:\n",
    "                                cv_scores_without_zeros[k][center_name][name][metric].append(calc_metric)\n",
    "                        else:\n",
    "                            calc_metric = roc_auc_score(y_true=y_train_sets[center_name].iloc[outer_valid_idx.tolist()], y_score=y_pred, average = 'macro')\n",
    "                            cv_scores[k][center_name][name][metric].append(calc_metric)\n",
    "                            if calc_metric != 0:\n",
    "                                cv_scores_without_zeros[k][center_name][name][metric].append(calc_metric)\n",
    "\n",
    "\n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277de3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the results\n",
    "\n",
    "\n",
    "mean_std_cv_scores = {k: {center_name: {name: {metric: {'mean': None, 'SD': None} for metric in metrics} for name, gs_est in gridcvs.items()} for center_name in center_names} for k in list_of_k}\n",
    "mean_std_cv_scores_without_zeros = {k: {center_name: {name: {metric: {'mean': None, 'SD': None} for metric in metrics} for name, gs_est in gridcvs.items()} for center_name in center_names} for k in list_of_k}\n",
    "median_iqr_cv_scores = {k: {center_name: {name: {metric: {'median': None, 'IQR': None} for metric in metrics} for name, gs_est in gridcvs.items()} for center_name in center_names} for k in list_of_k}\n",
    "median_iqr_cv_scores_without_zeros = {k: {center_name: {name: {metric: {'median': None, 'IQR': None} for metric in metrics} for name, gs_est in gridcvs.items()} for center_name in center_names} for k in list_of_k}\n",
    "\n",
    "for k in list_of_k:\n",
    "    \n",
    "    for center_name in center_names:\n",
    "    \n",
    "        for name, gs_est in sorted(gridcvs.items()):\n",
    "            print('Results for ' + center_name.upper() + ' ' + str(k) + ' ' + name.upper())\n",
    "            for metric in metrics: \n",
    "                if metric != 'auc':\n",
    "                    print('%-8s | outer CV ' + metric + ' %.2f%% +\\- %.3f' % (\n",
    "                      100 * np.mean(cv_scores[k][center_name][name][metric]), 100 * np.std(cv_scores[k][center_name][name][metric])))\n",
    "                    mean_std_cv_scores[k][center_name][name][metric]['mean'] = np.mean(cv_scores[k][center_name][name][metric])\n",
    "                    mean_std_cv_scores_without_zeros[k][center_name][name][metric]['mean'] = np.mean(cv_scores_without_zeros[k][center_name][name][metric])\n",
    "                    mean_std_cv_scores[k][center_name][name][metric]['SD'] = np.std(cv_scores[k][center_name][name][metric])\n",
    "                    mean_std_cv_scores_without_zeros[k][center_name][name][metric]['SD'] = np.std(cv_scores_without_zeros[k][center_name][name][metric])\n",
    "                    \n",
    "                    median_iqr_cv_scores[k][center_name][name][metric]['median'] = 100*np.median(cv_scores[k][center_name][name][metric])\n",
    "                    median_iqr_cv_scores_without_zeros[k][center_name][name][metric]['median'] = 100*np.median(cv_scores_without_zeros[k][center_name][name][metric])\n",
    "                    median_iqr_cv_scores[k][center_name][name][metric]['IQR'] = 100*iqr(cv_scores[k][center_name][name][metric])\n",
    "                    median_iqr_cv_scores_without_zeros[k][center_name][name][metric]['IQR'] = 100*iqr(cv_scores_without_zeros[k][center_name][name][metric])\n",
    "                else: \n",
    "                    print('%-8s | outer CV ' + metric + ' %.2f +\\- %.3f' % (\n",
    "                      np.mean(cv_scores[k][center_name][name][metric]), np.std(cv_scores[k][center_name][name][metric])))\n",
    "                    mean_std_cv_scores[k][center_name][name][metric]['mean'] = np.mean(cv_scores[k][center_name][name][metric])\n",
    "                    mean_std_cv_scores_without_zeros[k][center_name][name][metric]['mean'] = np.mean(cv_scores_without_zeros[k][center_name][name][metric])\n",
    "                    mean_std_cv_scores[k][center_name][name][metric]['SD'] = np.std(cv_scores[k][center_name][name][metric])\n",
    "                    mean_std_cv_scores_without_zeros[k][center_name][name][metric]['SD'] = np.std(cv_scores_without_zeros[k][center_name][name][metric])\n",
    "                    \n",
    "                    median_iqr_cv_scores[k][center_name][name][metric]['median'] = 100*np.median(cv_scores[k][center_name][name][metric])\n",
    "                    median_iqr_cv_scores_without_zeros[k][center_name][name][metric]['median'] = 100*np.median(cv_scores_without_zeros[k][center_name][name][metric])\n",
    "                    median_iqr_cv_scores[k][center_name][name][metric]['IQR'] = 100*iqr(cv_scores[k][center_name][name][metric])\n",
    "                    median_iqr_cv_scores_without_zeros[k][center_name][name][metric]['IQR'] = 100*iqr(cv_scores_without_zeros[k][center_name][name][metric])\n",
    "        print('*********************')\n",
    "#print('\\nSoftmax Best parameters', gridcvs['Softmax'].best_params_)\n",
    "#print('\\nKNN Best parameters', gridcvs['KNN'].best_params_)\n",
    "#print('\\nDTree Best parameters', gridcvs['DTree'].best_params_)\n",
    "#print('\\nSVM Best parameters', gridcvs['SVM'].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdbb759",
   "metadata": {},
   "outputs": [],
   "source": [
    "confedence_interval = {k: {center_name: {name: {metric: () for metric in metrics} for name, gs_est in gridcvs.items()} for center_name in center_names} for k in list_of_k}\n",
    "\n",
    "dof = 5-1 \n",
    "confidence = 0.95\n",
    "\n",
    "for k in list_of_k:\n",
    "    for center_name in center_names:\n",
    "        for name, gs_est in sorted(gridcvs.items()):\n",
    "            for metric in metrics:\n",
    "\n",
    "                m = mean_std_cv_scores[k][center_name][name][metric]['mean']\n",
    "                s = mean_std_cv_scores[k][center_name][name][metric]['SD']\n",
    "\n",
    "                t_crit = np.abs(t.ppf((1-confidence)/2,dof))\n",
    "                confedence_interval[k][center_name][name][metric]= ((m-s*t_crit/np.sqrt(5))*100, (m+s*t_crit/np.sqrt(5))*100)\n",
    "            \n",
    "confedence_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0695cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc_models_onlyinner = {metric: {name: {'mean': [], 'std': []} for name, gs_est in sorted(gridcvs.items())} for metric in metrics}\n",
    "\n",
    "for k in list_of_k:\n",
    "    for name, gs_est in sorted(gridcvs.items()):\n",
    "        for metric in metrics:\n",
    "            cv_acc_models_onlyinner[metric][name]['mean'].append(cv_scores_onlyinner[k]['all'][name][metric]['mean'])\n",
    "            cv_acc_models_onlyinner[metric][name]['std'].append(cv_scores_onlyinner[k]['all'][name][metric]['std'])\n",
    "cv_acc_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, gs_est in sorted(gridcvs.items()):\n",
    "    plt.figure()\n",
    "    #plt.scatter(list_of_k, cv_acc_models['accuracy'][name]['mean'])\n",
    "    plt.errorbar(x=list_of_k, y=cv_acc_models_onlyinner['accuracy'][name]['mean'], yerr=cv_acc_models_onlyinner['accuracy'][name]['std'], linestyle='None', marker='o')\n",
    "    plt.xlabel('number of features')\n",
    "    plt.ylabel('cv accuracy')\n",
    "    plt.title('cv accuracy for ' + name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22347f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_features_ids = {k: {center_name: [] for center_name in center_names} for k in list_of_k}\n",
    "merged_features_names = {k: {center_name: [] for center_name in center_names} for k in list_of_k}\n",
    "\n",
    "for k in list_of_k:\n",
    "    \n",
    "    for center_name in center_names:\n",
    "    \n",
    "        #print(center_name)\n",
    "        for i in range(len(features_ids[k][center_name])):\n",
    "            #print(i)\n",
    "            length = len(features_ids[k][center_name][i])\n",
    "            #print(length)\n",
    "            for j in range(length):\n",
    "                merged_features_ids[k][center_name].append(features_ids[k][center_name][i][j])\n",
    "                merged_features_names[k][center_name].append(features_names[k][center_name][i][j])\n",
    "\n",
    "merged_features_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601781de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list_of_k:\n",
    "    for center_name in center_names: \n",
    "        print('Number of features: ' + str(k))\n",
    "        print(len(merged_features_ids[k][center_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0cd20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "counts = {k: {center_name: {} for center_name in center_names} for k in list_of_k}\n",
    "\n",
    "for k in list_of_k: \n",
    "    print('Number of features: ' + str(k))\n",
    "    for center_name in center_names:\n",
    "        print(center_name)\n",
    "        \n",
    "        for feature_name in merged_features_names[k][center_name]:\n",
    "            #print(feature_name)\n",
    "            count = merged_features_names[k][center_name].count(feature_name)\n",
    "            #print(count)\n",
    "            counts[k][center_name].update({feature_name: count})\n",
    "\n",
    "        counts[k][center_name] = sorted(counts[k][center_name].items(),key=operator.itemgetter(1),reverse=True)\n",
    "        print(len(counts[k][center_name]))\n",
    "        \n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dabce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list_of_k:\n",
    "    for center_name in center_names:\n",
    "        print(center_name)\n",
    "        print('Number of features: ' + str(k))\n",
    "        merged_features_ids[k][center_name] = list(set(merged_features_ids[k][center_name]))\n",
    "        print(len(merged_features_ids[k][center_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39338231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOTSTRAPPING\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Fitting a model to the whole training set\n",
    "# using the \"best\" algorithm\n",
    "best_algo = gridcvs['Softmax']\n",
    "\n",
    "\n",
    "bootstrap = {center_name: {metric: [] for metric in metrics} for center_name in center_names}\n",
    "\n",
    "for center_name in center_names:\n",
    "    \n",
    "    print('Test results for ' + center_name.upper())\n",
    "    \n",
    "    #Lets configure Bootstrap\n",
    "    \n",
    "    print(X_test_sets[center_name].shape)\n",
    "\n",
    "    n_iterations = 100  # No. of bootstrap samples to be repeated (created)\n",
    "    n_size = int(len(X_test_sets[center_name]) * 1.0) # Size of sample, picking only 50% of the given data in every bootstrap sample\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "    \n",
    "        test_values = resample(X_test_sets[center_name], n_samples = n_size)\n",
    "        print(test_values.shape)\n",
    "        print(test_values.index.to_list())\n",
    "        test_labels = y_test_sets[center_name].loc[test_values.index.to_list()]\n",
    "\n",
    "\n",
    "        if center_name == 'all':\n",
    "            start = time.time()\n",
    "            best_algo.fit(X_train_sets[center_name].iloc[:,merged_features_ids[67]['all']], y_train_sets[center_name].values.ravel())\n",
    "            stop = time.time()\n",
    "\n",
    "            for metric in metrics: \n",
    "\n",
    "                if metric == 'accuracy':\n",
    "                    calc_metric = accuracy_score(y_true=test_labels, y_pred=best_algo.predict(test_values.iloc[:,merged_features_ids[67]['all']]))\n",
    "                    print('Test Accuracy: %.2f%%' % (100 * calc_metric))\n",
    "                    bootstrap[center_name][metric].append(calc_metric)\n",
    "                elif metric == 'recall':\n",
    "                    calc_metric = recall_score(y_true=test_labels, y_pred=best_algo.predict(test_values.iloc[:,merged_features_ids[67]['all']]))\n",
    "                    print('Recall/sensitivity: %.2f%%' % (100 * calc_metric))\n",
    "                    bootstrap[center_name][metric].append(calc_metric)\n",
    "                elif metric == 'specificity':\n",
    "                    calc_metric = specificity_score(y_true=test_labels, y_pred=best_algo.predict(test_values.iloc[:,merged_features_ids[67]['all']]))\n",
    "                    print('Specificity: %.2f%%' % (100 * calc_metric))\n",
    "                    bootstrap[center_name][metric].append(calc_metric)\n",
    "                elif metric == 'precision':\n",
    "                    calc_metric = precision_score(y_true=test_labels, y_pred=best_algo.predict(test_values.iloc[:,merged_features_ids[67]['all']]))\n",
    "                    print('Precision: %.2f%%' % (100 * calc_metric))\n",
    "                    bootstrap[center_name][metric].append(calc_metric)\n",
    "                elif metric == 'F1':\n",
    "                    calc_metric = f1_score(y_true=test_labels, y_pred=best_algo.predict(test_values.iloc[:,merged_features_ids[67]['all']]))\n",
    "                    print('F1 score: %.2f%%' % (100 * calc_metric))\n",
    "                    bootstrap[center_name][metric].append(calc_metric)\n",
    "                else:\n",
    "                    calc_metric = roc_auc_score(y_true=test_labels, y_score=best_algo.predict(test_values.iloc[:,merged_features_ids[67]['all']]), average = 'macro')\n",
    "                    print('ROC AUC: %.2f' % calc_metric)\n",
    "                    bootstrap[center_name][metric].append(calc_metric)\n",
    "\n",
    "        else:\n",
    "\n",
    "            for metric in metrics: \n",
    "\n",
    "                if metric == 'accuracy':\n",
    "                    calc_metric = accuracy_score(y_true=test_labels, y_pred=best_algo.predict(test_values.iloc[:,merged_features_ids[67]['all']]))\n",
    "                    print('Test Accuracy: %.2f%%' % (100 * calc_metric))\n",
    "                    bootstrap[center_name][metric].append(calc_metric)\n",
    "                elif metric == 'recall':\n",
    "                    calc_metric = recall_score(y_true=test_labels, y_pred=best_algo.predict(test_values.iloc[:,merged_features_ids[67]['all']]))\n",
    "                    print('Recall/sensitivity: %.2f%%' % (100 * calc_metric))\n",
    "                    bootstrap[center_name][metric].append(calc_metric)\n",
    "                elif metric == 'specificity':\n",
    "                    calc_metric = specificity_score(y_true=test_labels, y_pred=best_algo.predict(test_values.iloc[:,merged_features_ids[67]['all']]))\n",
    "                    print('Specificity: %.2f%%' % (100 * calc_metric))\n",
    "                    bootstrap[center_name][metric].append(calc_metric)\n",
    "                elif metric == 'precision':\n",
    "                    calc_metric = precision_score(y_true=test_labels, y_pred=best_algo.predict(test_values.iloc[:,merged_features_ids[67]['all']]))\n",
    "                    print('Precision: %.2f%%' % (100 * calc_metric))\n",
    "                    bootstrap[center_name][metric].append(calc_metric)\n",
    "                elif metric == 'F1':\n",
    "                    calc_metric = f1_score(y_true=test_labels, y_pred=best_algo.predict(test_values.iloc[:,merged_features_ids[67]['all']]))\n",
    "                    print('F1 score: %.2f%%' % (100 * calc_metric))\n",
    "                    bootstrap[center_name][metric].append(calc_metric)\n",
    "                else:\n",
    "                    calc_metric = roc_auc_score(y_true=test_labels, y_score=best_algo.predict(test_values.iloc[:,merged_features_ids[67]['all']]), average = 'macro')\n",
    "                    print('ROC AUC: %.2f' % calc_metric)\n",
    "                    bootstrap[center_name][metric].append(calc_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5075ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for center_name in center_names:\n",
    "    print('Results for ' + center_name.upper())\n",
    "    for metric in metrics: \n",
    "        if metric != 'auc':\n",
    "            print(metric + ' %.2f%% +\\- %.3f' % (\n",
    "              100 * np.mean(bootstrap[center_name][metric]), 100 * np.std(bootstrap[center_name][metric])))\n",
    "        else: \n",
    "            print(metric + ' %.2f +\\- %.3f' % (\n",
    "              np.mean(bootstrap[center_name][metric]), np.std(bootstrap[center_name][metric])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
